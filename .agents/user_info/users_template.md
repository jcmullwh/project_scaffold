# Users

> Purpose: Capture *intent* (why someone reaches for this repo, what they’re trying to decide/do, and what counts as evidence), and provide a foundation for discovery-oriented user simulation.
>
> This is not a spec of exact steps. It should stay valid even as the UI/API changes.

## Status
- Owner(s): [team / role]
- Last reviewed: [YYYY-MM-DD]
- Scope: [library / CLI / service / template / plugin]
- Where this file is used: [prioritization / agentic simulation / docs roadmap / release checklist]

---

## Why this project exists

### Motivating problems (the “before” cost)
Describe the real-world costs that cause someone to look for a project like this.

- [Problem / cost #1]
- [Problem / cost #2]
- [Problem / cost #3]

### What this project is (in one sentence)
- [One sentence statement of what it enables]

### Non-goals
Explicitly state what this project is *not* trying to do (prevents accidental scope creep).

- [Non-goal #1]
- [Non-goal #2]

---

## Who this is for

> Personas here are “roles + context.” Avoid assuming exact tactics.

For each persona, fill in only what you actually know; leave “Unknowns” when you don’t.

### Persona: [Name]
- Situation / trigger:
  - [When they reach for this repo]
- Job to be done:
  - [What they are trying to accomplish or decide]
- Motivating pain (why they would adopt this category of tool):
  - [Cost or risk they want to reduce]
- Constraints (shaping context, not complaints):
  - [Org / security / environment / time / risk posture]
- What counts as evidence (to decide it’s a fit):
  - [Signals they trust: artifacts, outputs, integrations, invariants]
- Deal-breakers:
  - [What makes them abandon]
- Unknowns (things we suspect but haven’t validated):
  - [Unknown #1]
  - [Unknown #2]

(Add more personas as needed.)

---

## What they care about

> Prefer outcomes, tradeoffs, and decision criteria over UX opinions.

### Outcomes
- [Outcome #1]
- [Outcome #2]

### Acceptable tradeoffs
- [Tradeoff they accept]
- [Tradeoff they won’t accept]

### Decision criteria (how they judge fit)
- [Criterion #1]
- [Criterion #2]

---

## How they interact with the project

> Interaction modes, not scripts.

- Entry points:
  - [README / examples / CLI / API / templates / integrations]
- Adoption modes:
  - [incremental vs big-bang] / [local-first vs CI-first]
- Typical working style:
  - [explore by modifying an example] / [integrate into existing repo] / [evaluate in sandbox]

---

## Critical missions

> “Missions” are intent-based journeys: what they’re trying to decide/do, with room for variation.

### Mission: [Name]
- Intent:
  - [What decision/job this mission represents]
- Starting state:
  - [Clean slate assumptions, e.g., fresh clone, no prior config]
- Variations worth exploring:
  - [Environment / constraints / integration target variations]
- What counts as success (evidence):
  - [What would let a reasonable user decide “fit / not fit / unsure”]
- Likely forks:
  - [Big branching choices users make]
- Failure meaning:
  - [If this fails, what might it imply? docs gap vs true incompatibility vs missing feature]
- Unknowns this mission is meant to surface:
  - [Surprise types you want to discover]

(Add missions as needed.)

---

## User-journey metrics (observed signals)

> No budgets here by default. These are measurements used to learn and compare over time.

- Time-to-decision:
  - Definition: [when the user can credibly say fit / not fit / unsure with evidence]
- Evidence coverage:
  - Definition: [fraction of key claims backed by repo/docs/output references]
- Backtracking / reversals:
  - Definition: [times a user undoes a decision or switches approach]
- Fix-forward cost:
  - Definition: [effort to recover from mistakes / get unblocked]
- Surprise discovery rate:
  - Definition: [new constraints, hidden assumptions, undocumented requirements surfaced]
- Doc reliance map:
  - Definition: [which docs were consulted, and at what decision point]

---

## Open questions & unknowns backlog
- [Open question #1]
- [Open question #2]
